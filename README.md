# NNLib â€”Â Building NeuralÂ NetworksÂ fromÂ Scratch

> **Handsâ€‘on playground for understanding neuralâ€‘net internals.** From a singleâ€‘neuron demo to a Tritonâ€‘accelerated matrixâ€‘mult, every directory captures a new learning milestone.

---

## ğŸ“ Project layout

```
NNLib/
â”œâ”€â”€ nn.py                # âŸµ runnable demo using v1 API
â”œâ”€â”€ simple_nn.py         # âŸµ very first singleâ€‘hiddenâ€‘layer experiment
â”œâ”€â”€ nn_lib.py            # âŸµ v1: pureâ€‘Python / NumPy implementation
â”œâ”€â”€ nn_lib2.py           # âŸµ v2: TorchÂ + Triton kernels
â”œâ”€â”€ functions/           # numerical utils
â”‚   â”œâ”€â”€ activation.py    #   activation fn & derivatives (Torch)
â”‚   â””â”€â”€ loss.py          #   leastâ€‘squares + derivative
â”œâ”€â”€ kernels/             # Triton GPU kernels used by v2
â”‚   â”œâ”€â”€ activations.py   #   elementâ€‘wise activations
â”‚   â”œâ”€â”€ add.py           #   vector addition
â”‚   â”œâ”€â”€ feedforward.py   #   tiled MatMul + bias + activation
â”‚   â”œâ”€â”€ matmul.py        #   baseline matmul (WIP)
â”‚   â”œâ”€â”€ point_multiply.py
â”‚   â””â”€â”€ subtract.py
â””â”€â”€ TODO.txt
```

## ğŸ”§ Installation

### 1. Clone & create env

```bash
$ git clone https://github.com/<you>/NNLib.git
$ cd NNLib
$ python -m venv .venv && source .venv/bin/activate
```

### 2. Pick your flavour

| Want to runâ€¦                       | Install                                                                |
| ---------------------------------- | ---------------------------------------------------------------------- |
| only `simple_nn.py` or `nn_lib.py` | `pip install numpy matplotlib tqdm`                                    |
| `nn_lib2.py` on **CPU**            | `pip install torch numpy matplotlib tqdm`                              |
| `nn_lib2.py` on **NVIDIAÂ GPU**     | `pip install torch --index-url https://download.pytorch.org/whl/cu118` |
|                                    | `pip install triton`                                                   |

> **note:** Skip Triton if youâ€™re not on a CUDAâ€‘capable GPU

---

## ğŸš€ Quick start

### Pure NumPy network (v1)

Run it:

```bash
$ python nn.py
```

A window pops up showing how the learned curve fits the data.

### Tritonâ€‘accelerated network (v2)

```python
from nn_lib2 import NeuralNetwork

net = NeuralNetwork(layers=[(2, None), (64, "leaky_relu"), (32, "leaky_relu"), (1, None)],
                    learning_rate=1eâ€‘3)

# batch of 128 random samples
import torch, math
X = torch.rand(128, 2)
y_true = torch.sin(X[:,0]*math.pi) + torch.cos(X[:,1]*math.pi)

for _ in range(2000):
    y_pred = net.feed_forward(X.tolist())
    loss   = (y_pred.squeeze() - y_true).pow(2).mean()
    net.backpropagate(loss.unsqueeze(0))
```
---

## ğŸ§  What I learned building this

- **Manual backâ€‘prop math**: starting with perâ€‘parameter gradients before vectorising.
- **Stability tricks**: smallâ€‘weight initialisation & learningâ€‘rate scheduling.
- **GPU kernels**: how tiled MatMul, bias broadcast, and activation fuse for memory efficiency.

---
